{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice with PyTorch: LSTMs and Spam/Ham Classifiers\n",
    "In this tutorial, we'll practice with using PyTorch and [Long Short Term Memory (LSTM)](https://en.wikipedia.org/wiki/Long_short-term_memory) Neural Network models to build a spam classifier for emails.\n",
    "\n",
    "![emails](notebook_diagrams/email_classifier.png)\n",
    "\n",
    "The reference for this tutorial can be found [here](https://github.com/sijoonlee/spam-ham-walkthrough/blob/master/walkthrough.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Install PyTorch\n",
    "We'll use Anaconda to install PyTorch on our AWS machines for this tutorial.  If you don't want to install this package through Anaconda, you can also do so through `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate conda environment\n",
    "! conda activate local_env\n",
    "\n",
    "# Install PyTorch in Conda environment\n",
    "! conda install -c pytorch pytorch\n",
    "! pip install torchvision\n",
    "\n",
    "# Check PyTorch version\n",
    "! pip show torch\n",
    "\n",
    "# Use matplotlib inline version\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Import Packages\n",
    "Here, since we're processing a lot of (possibly invalid) text data, we'll make use of the `pandas` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "owAIBuR0SYgd"
   },
   "outputs": [],
   "source": [
    "# For reading file paths\n",
    "import os\n",
    "\n",
    "# For processing data\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xXs-riOaHPKt"
   },
   "outputs": [],
   "source": [
    "# You can download the data here: http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/index.html\n",
    "\n",
    "# Download data\n",
    "!wget http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron1.tar.gz\n",
    "!wget http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron2.tar.gz\n",
    "!wget http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron3.tar.gz\n",
    "!wget http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron4.tar.gz\n",
    "!wget http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron5.tar.gz\n",
    "!wget http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron6.tar.gz\n",
    "\n",
    "# Now unzip the data into the current directory\n",
    "!tar -zxvf enron1.tar.gz\n",
    "!tar -zxvf enron2.tar.gz\n",
    "!tar -zxvf enron3.tar.gz\n",
    "!tar -zxvf enron4.tar.gz\n",
    "!tar -zxvf enron5.tar.gz\n",
    "!tar -zxvf enron6.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-Processing\n",
    "Like our computer vision applications, pre-processing of data will be important for this email classification problem as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Define Our File Reader \n",
    "We'll use our file reader to create our training and testing data for this neural network exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "class File_reader(object):\n",
    "  def __init__(self):\n",
    "    self.ham = []\n",
    "    self.spam = []\n",
    "    self.ham_paths = [\"enron1/ham/*.txt\", \"enron2/ham/*.txt\", \"enron3/ham/*.txt\", \"enron4/ham/*.txt\", \"enron5/ham/*.txt\", \"enron6/ham/*.txt\"]\n",
    "    self.spam_paths = [\"enron1/spam/*.txt\", \"enron2/spam/*.txt\", \"enron3/spam/*.txt\", \"enron4/spam/*.txt\", \"enron5/spam/*.txt\", \"enron6/spam/*.txt\"]\n",
    "\n",
    "  def read_file(self, path, minimum_word_count = 3, unnecessary =  [\"-\", \".\", \",\", \"/\", \":\", \"@\"]):\n",
    "    files  = glob.glob(path)\n",
    "    content_list = []\n",
    "    for file in files:\n",
    "        with open(file, encoding=\"ISO-8859-1\") as f:\n",
    "            content = f.read()\n",
    "            if len(content.split()) > minimum_word_count:\n",
    "              content = content.lower()\n",
    "              if len(unnecessary) is not 0:\n",
    "                  content = ''.join([c for c in content if c not in unnecessary])\n",
    "              content_list.append(content)\n",
    "    return content_list\n",
    "\n",
    "  def cut_before_combine(self, data, max = 5000):\n",
    "    if max is not 0:\n",
    "      if len(data) > max:\n",
    "        random.shuffle(data)\n",
    "        data = data[:max]\n",
    "    return data\n",
    "\n",
    "  def load_ham_and_spam(self, ham_paths = \"default\", spam_paths = \"default\", max = 5000): # 0 for no truncation\n",
    "\n",
    "    if ham_paths == \"default\":\n",
    "      ham_paths = self.ham_paths\n",
    "    if spam_paths == \"default\":\n",
    "      spam_paths = self.spam_paths\n",
    "\n",
    "    self.ham = [ item for path in ham_paths for item in self.read_file(path) ]\n",
    "    if max != 0:\n",
    "      self.ham = self.cut_before_combine(self.ham, max)\n",
    "    print(\"ham length \", len(self.ham))\n",
    "\n",
    "    self.spam = [item for path in spam_paths for item in self.read_file(path) ]\n",
    "    if max != 0:\n",
    "      self.spam = self.cut_before_combine(self.spam, max)\n",
    "    print(\"spam length \", len(self.spam))\n",
    "\n",
    "    data = self.ham + self.spam\n",
    "\n",
    "    ham_label = [0 for _ in range(len(self.ham))]\n",
    "    spam_label = [1 for _ in range(len(self.spam))]\n",
    "\n",
    "    label_tensor = torch.as_tensor(ham_label + spam_label, dtype = torch.int16)\n",
    "\n",
    "    return data, label_tensor\n",
    "\n",
    "  def print_sample(self, which =\"both\"): # ham, spam or both\n",
    "    if which == \"ham\" or which == \"both\":\n",
    "      idx = random.randint(0, len(self.ham))\n",
    "      print(\"----------- ham sample -------------\")\n",
    "      print(self.ham[idx])\n",
    "    if which == \"spam\" or which == \"both\":\n",
    "      idx = random.randint(0, len(self.spam))\n",
    "      print(\"----------- spam sample -------------\")\n",
    "      print(self.spam[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "yG9yAN2qHqv2",
    "outputId": "543bdb14-6fc9-44d3-8ed1-fad82e92b085"
   },
   "outputs": [],
   "source": [
    "# Make file reader object\n",
    "fr = File_reader()\n",
    "\n",
    "# Use file reader object to get data and labels\n",
    "data, label = fr.load_ham_and_spam(ham_paths = \"default\", spam_paths = \"default\", max = 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define Vocabulary Objects for Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = [vocab for seq in data for vocab in seq.split()]\n",
    "# a = [  word for seq in [\"a d\",\"b d\",\"c d\"] for word in seq.split() ]\n",
    "# ['a', 'd', 'b', 'd', 'c', 'd']\n",
    "\n",
    "vocab_count = Counter(vocabs)\n",
    "# Count words in the whole dataset\n",
    "\n",
    "print(vocab_count)\n",
    "# Counter({'the': 47430, 'to': 35684, 'and': 26245, 'of': 24176, 'a': 19290, 'in': 17442, 'you': 14258, ...\n",
    "\n",
    "vocab_count = vocab_count.most_common(len(vocab_count))\n",
    "\n",
    "vocab_to_int = {word : index+2 for index, (word, count) in enumerate(vocab_count)}\n",
    "vocab_to_int.update({'__PADDING__': 0}) # index 0 for padding\n",
    "vocab_to_int.update({'__UNKNOWN__': 1}) # index 1 for unknown word such as broken character\n",
    "\n",
    "print(vocab_to_int)\n",
    "# {'the': 2, 'to': 3, 'and': 4, 'of': 5, 'a': 6, 'in': 7, 'you': 8, 'for': 9, \"'\": 10, 'is': 11, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Notice how balanced the dataset above is!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Feature Engineering: Tokenization and Vectorization of Text Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qZ8bboqcgJSb"
   },
   "outputs": [],
   "source": [
    "# Import pytorch package and important modules\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Tokenize & Vectorize sequences\n",
    "vectorized_seqs = []\n",
    "for seq in data: \n",
    "  vectorized_seqs.append([vocab_to_int[word] for word in seq.split()])\n",
    "\n",
    "# Save the lengths of sequences\n",
    "seq_lengths = torch.LongTensor(list(map(len, vectorized_seqs)))\n",
    "\n",
    "# Add padding(0)\n",
    "seq_tensor = Variable(torch.zeros((len(vectorized_seqs), seq_lengths.max()))).long()\n",
    "for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "  seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "  \n",
    "\n",
    "print(seq_lengths.max()) # tensor(30772)\n",
    "print(seq_tensor[0]) # tensor([ 20,  77, 666,  ...,   0,   0,   0])\n",
    "print(seq_lengths[0]) # tensor(412)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "87Ma75fIiLeS",
    "outputId": "5e84e2c2-1757-4f68-d7da-304c3a646ca3"
   },
   "outputs": [],
   "source": [
    "sample = \"operations is digging out 2000 feet of pipe to begin the hydro test\"\n",
    "\n",
    "tokenized_sample = [ word for word in sample.split()]\n",
    "print(tokenized_sample[:3]) # ['operations', 'is', 'digging']\n",
    "\n",
    "vectorized_sample = [ vocab_to_int.get(word, 1) for word in tokenized_sample] # unknown word in dict marked as 1\n",
    "print(vectorized_sample[:3]) # [424, 11, 14683]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Define Our PyTorch DataLoader\n",
    "DataLoaders are extremely important objects in PyTorch.  They are tools we can use for easily customizing how our data is ingested during training and evaluation, and enable for compact, well-defined data augmentation.  For a tutorial on how DataLoaders work, visit the link [here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
    "\n",
    "**NOTE**: Nearly all `DataLoader` objects are different from one another, so it's not critical that you memorize the structure below.  It is only important to know the methods that are used that define the `DataLoader`, and what they are used for:\n",
    "\n",
    "- `__init__`: This is the constructor method called when this kind of `DataLoader` object is created (instantiated).\n",
    "\n",
    "\n",
    "- `__iter__`:  This method defines how the DataLoader iterates through tensors it's given.\n",
    "\n",
    "\n",
    "- `_next_index`: This method defines how the next index of the DataLoader is found.\n",
    "\n",
    "\n",
    "- `__next__`: This method defines how the next element in the DataLoader is returned.\n",
    "\n",
    "\n",
    "- `__len__`: This method defines the length of the DataLoader for iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bc71-icLjIwI"
   },
   "outputs": [],
   "source": [
    "# Import data sampler from pytorch\n",
    "import torch.utils.data.sampler as splr\n",
    "\n",
    "# Create custom DataLoader that we'll use for loading training data into our training pipeline\n",
    "class CustomDataLoader(object):\n",
    "    \n",
    "  # Constructor method\n",
    "  def __init__(self, seq_tensor, seq_lengths, label_tensor, batch_size):\n",
    "    self.batch_size = batch_size\n",
    "    self.seq_tensor = seq_tensor\n",
    "    self.seq_lengths = seq_lengths\n",
    "    self.label_tensor = label_tensor\n",
    "    self.sampler = splr.BatchSampler(splr.RandomSampler(self.label_tensor), self.batch_size, False)\n",
    "    self.sampler_iter = iter(self.sampler)\n",
    "  \n",
    "  # This method defines how the DataLoader iterates\n",
    "  def __iter__(self):\n",
    "    self.sampler_iter = iter(self.sampler) # reset sampler iterator\n",
    "    return self\n",
    "  \n",
    "  # This method defines how the next index of the DataLoader is found\n",
    "  def _next_index(self):\n",
    "    return next(self.sampler_iter) # may raise StopIteration\n",
    "  \n",
    "  # This method defines how the next element in the DataLoader is returned\n",
    "  def __next__(self):\n",
    "    index = self._next_index()\n",
    "\n",
    "    subset_seq_tensor = self.seq_tensor[index]\n",
    "    subset_seq_lengths = self.seq_lengths[index]\n",
    "    subset_label_tensor = self.label_tensor[index]\n",
    "\n",
    "    # order by length to use pack_padded_sequence()\n",
    "    subset_seq_lengths, perm_idx = subset_seq_lengths.sort(0, descending=True)\n",
    "    subset_seq_tensor = subset_seq_tensor[perm_idx]\n",
    "    subset_label_tensor = subset_label_tensor[perm_idx]\n",
    "\n",
    "    return subset_seq_tensor, subset_seq_lengths, subset_label_tensor\n",
    "\n",
    "  # This method defines the length of the DataLoader for iteration\n",
    "  def __len__(self):\n",
    "    return len(self.sampler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Split Data into Training and Testing\n",
    "As with the other machine learning frameworks we've analyzed ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "K5mlBcf_hbhA",
    "outputId": "20de31bb-aed6-417c-dfff-3059387b3386"
   },
   "outputs": [],
   "source": [
    "shuffled_idx = torch.randperm(label.shape[0])\n",
    "\n",
    "seq_tensor = seq_tensor[shuffled_idx]\n",
    "seq_lenghts = seq_lengths[shuffled_idx]\n",
    "label = label[shuffled_idx]\n",
    "\n",
    "PCT_TRAIN = 0.7\n",
    "PCT_VALID = 0.2\n",
    "\n",
    "length = len(label)\n",
    "\n",
    "# Specify components of training dataset\n",
    "train_seq_tensor = seq_tensor[:int(length*PCT_TRAIN)] \n",
    "train_seq_lengths = seq_lengths[:int(length*PCT_TRAIN)]\n",
    "train_label = label[:int(length*PCT_TRAIN)]\n",
    "\n",
    "# Specify components of validation dataset\n",
    "valid_seq_tensor = seq_tensor[int(length*PCT_TRAIN):int(length*(PCT_TRAIN+PCT_VALID))] \n",
    "valid_seq_lengths = seq_lengths[int(length*PCT_TRAIN):int(length*(PCT_TRAIN+PCT_VALID))] \n",
    "valid_label = label[int(length*PCT_TRAIN):int(length*(PCT_TRAIN+PCT_VALID))]\n",
    "\n",
    "# Specify components of testing dataset\n",
    "test_seq_tensor = seq_tensor[int(length*(PCT_TRAIN+PCT_VALID)):]\n",
    "test_seq_lengths = seq_lengths[int(length*(PCT_TRAIN+PCT_VALID)):]\n",
    "test_label = label[int(length*(PCT_TRAIN+PCT_VALID)):]\n",
    "\n",
    "# Display datasets\n",
    "print(train_seq_tensor.shape) # torch.Size([4200, 30772])\n",
    "print(valid_seq_tensor.shape) # torch.Size([1199, 30772])\n",
    "print(test_seq_tensor.shape) # torch.Size([601, 30772])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Set Batch Size and Create DataLoaders\n",
    "We can use our `CustomDataLoader` class defined above as our dataloader for this problem.  **NOTE**: We need to give these DataLoaders a batch size for them to be used in our training procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A3JReWZStM0p"
   },
   "outputs": [],
   "source": [
    "# set shuffle = False since data is already shuffled\n",
    "batch_size = 80\n",
    "\n",
    "# Create training data loader\n",
    "train_loader = CustomDataLoader(train_seq_tensor, train_seq_lengths, train_label, batch_size)\n",
    "\n",
    "# Create validation data loader\n",
    "valid_loader = CustomDataLoader(valid_seq_tensor, valid_seq_lengths, valid_label, batch_size)\n",
    "\n",
    "# Create testing data loader\n",
    "test_loader = CustomDataLoader(test_seq_tensor, test_seq_lengths, test_label, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define LSTM Model and Parameters\n",
    "\n",
    "### 2.1 Define LSTM Model\n",
    "As mentioned at the beginning of this tutorial, we'll be using an LSTM model to predict whether an email is \"spam\" or \"ham\".  Let's define our model below, using PyTorch!  Below is an example where having the ability to customize different features of the network is quite helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rEUW3ETgu6ub"
   },
   "outputs": [],
   "source": [
    "# Import nn module and RNN sub-module from PyTorch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Class for our LSTM model\n",
    "class SpamHamLSTM(nn.Module):\n",
    "    \n",
    "    # Constructor method for model\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size, n_layers,\\\n",
    "                 drop_lstm=0.1, drop_out = 0.1):\n",
    "        \n",
    "        # Model inherits from nn.Module superclass\n",
    "        super().__init__()\n",
    "        \n",
    "        # Specify other parameters\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embedding \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_lstm, batch_first=True)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        \n",
    "        # Linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    # Method for making predictions from inputs to outputs\n",
    "    def forward(self, x, seq_lengths):\n",
    "\n",
    "        # Embeddings\n",
    "        embedded_seq_tensor = self.embedding(x)\n",
    "                \n",
    "        # Pack, remove pads\n",
    "        packed_input = pack_padded_sequence(embedded_seq_tensor, seq_lengths.cpu().numpy(), batch_first=True)\n",
    "        \n",
    "        # LSTM\n",
    "        packed_output, (ht, ct) = self.lstm(packed_input, None)\n",
    "          # https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html\n",
    "          # If `(h_0, c_0)` is not provided, both **h_0** and **c_0** default to zero\n",
    "\n",
    "        # Unpack, recover padded sequence\n",
    "        output, input_sizes = pad_packed_sequence(packed_output, batch_first=True)\n",
    "       \n",
    "        # Collect the last output in each batch\n",
    "        last_idxs = (input_sizes - 1).to(device) # last_idxs = input_sizes - torch.ones_like(input_sizes)\n",
    "        output = torch.gather(output, 1, last_idxs.view(-1, 1).unsqueeze(2).repeat(1, 1, self.hidden_dim)).squeeze() # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Dropout and fully-connected layer\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output).squeeze()\n",
    "               \n",
    "        # Sigmoid function\n",
    "        output = self.sig(output)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Specify Model and Training Hyperparameters\n",
    "Now we can specify parameters that are critical for our LSTM model and training it effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "6OBmZsymvL3_",
    "outputId": "67f25104-1ea4-4198-f3bb-583069dc5121"
   },
   "outputs": [],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int)\n",
    "embedding_dim = 100 # int(vocab_size ** 0.25) # 15\n",
    "hidden_dim = 15\n",
    "output_size = 1\n",
    "n_layers = 2\n",
    "\n",
    "# See if we have GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "\n",
    "# Make network object using custom architecture from above\n",
    "net = SpamHamLSTM(vocab_size, embedding_dim, hidden_dim, output_size, n_layers, \\\n",
    "                 0.2, 0.2)\n",
    "\n",
    "# If we have GPU, move network from CPU --> GPU\n",
    "net = net.to(device)\n",
    "\n",
    "# Print network\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Specify Loss, Optimizer, and Scheduler\n",
    "These are important for training our network efficiently and effectively.\n",
    "\n",
    "**NOTE**: We didn't explicitly discuss schedulers in the previous tutorial, but if you're interested in learning more about them, you can do so [here](https://pytorch.org/docs/stable/optim.html).  Essentially, these objects enable for more stable training by dynamically adjusting the learning rate based off of validation dataset performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qDqRXRCs0cCB"
   },
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Learning rate and optimizer\n",
    "lr=0.03\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "# We didn't mention this before, but using schedulers are a way to achieve more stable training\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\\\n",
    "                                                       mode = 'min', \\\n",
    "                                                      factor = 0.5,\\\n",
    "                                                      patience = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Model\n",
    "Now that we've specified our model, our model hyperparameters, and our training hyperparameters, we are ready to train our email classifier on our data!\n",
    "\n",
    "### 3.1 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "colab_type": "code",
    "id": "bQwJdYMy0gdB",
    "outputId": "24480f6c-25fb-4de7-f4b8-6fc75dc3651e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# training params\n",
    "epochs = 6 \n",
    "counter = 0\n",
    "print_every = 10\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# Specify this to tell the network it needs to train\n",
    "net.train()\n",
    "\n",
    "# TRAINING LOOP - train for some number of epochs\n",
    "val_losses = []\n",
    "epochs_list = []\n",
    "for e in range(epochs):\n",
    "    \n",
    "    epochs_list.append(e)\n",
    "    \n",
    "    scheduler.step(e)\n",
    "\n",
    "    for seq_tensor, seq_tensor_lengths, label in iter(train_loader):\n",
    "        counter += 1\n",
    "               \n",
    "        seq_tensor = seq_tensor.to(device)\n",
    "        seq_tensor_lengths = seq_tensor_lengths.to(device)\n",
    "        label = label.to(device)\n",
    " \n",
    "        # get the output from the model\n",
    "        output = net(seq_tensor, seq_tensor_lengths)\n",
    "    \n",
    "        # get the loss and backprop\n",
    "        loss = criterion(output, label.float())\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        \n",
    "        # prevent the exploding gradient\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            \n",
    "            val_losses_in_itr = []\n",
    "            sums = []\n",
    "            sizes = []\n",
    "            \n",
    "            net.eval()\n",
    "            \n",
    "            for seq_tensor, seq_tensor_lengths, label in iter(valid_loader):\n",
    "\n",
    "                seq_tensor = seq_tensor.to(device)\n",
    "                seq_tensor_lengths = seq_tensor_lengths.to(device)\n",
    "                label = label.to(device)\n",
    "                output = net(seq_tensor, seq_tensor_lengths)\n",
    "                \n",
    "                # losses\n",
    "                val_loss = criterion(output, label.float())     \n",
    "                val_losses_in_itr.append(val_loss.item())\n",
    "                \n",
    "                # accuracy\n",
    "                binary_output = (output >= 0.5).short() # short(): torch.int16\n",
    "                right_or_not = torch.eq(binary_output, label)\n",
    "                sums.append(torch.sum(right_or_not).float().item())\n",
    "                sizes.append(right_or_not.shape[0])\n",
    "            \n",
    "            accuracy = sum(sums) / sum(sizes)\n",
    "            \n",
    "            net.train()\n",
    "            print(\"Epoch: {:2d}/{:2d}\\t\".format(e+1, epochs),\n",
    "                  \"Steps: {:3d}\\t\".format(counter),\n",
    "                  \"Loss: {:.6f}\\t\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\\t\".format(np.mean(val_losses_in_itr)),\n",
    "                  \"Accuracy: {:.3f}\".format(accuracy))\n",
    "            \n",
    "# Epoch:  1/ 6\t Steps:  10\t Loss: 0.693371\t Val Loss: 0.689860\t Accuracy: 0.530\n",
    "# Epoch:  1/ 6\t Steps:  20\t Loss: 0.699150\t Val Loss: 0.667903\t Accuracy: 0.585\n",
    "# Epoch:  1/ 6\t Steps:  30\t Loss: 0.631709\t Val Loss: 0.626028\t Accuracy: 0.651\n",
    "# Epoch:  1/ 6\t Steps:  40\t Loss: 0.609348\t Val Loss: 0.538908\t Accuracy: 0.716\n",
    "# Epoch:  1/ 6\t Steps:  50\t Loss: 0.435395\t Val Loss: 0.440515\t Accuracy: 0.780\n",
    "# Epoch:  2/ 6\t Steps:  60\t Loss: 0.364830\t Val Loss: 0.312334\t Accuracy: 0.892\n",
    "# Epoch:  2/ 6\t Steps:  70\t Loss: 0.177650\t Val Loss: 0.283867\t Accuracy: 0.901\n",
    "# Epoch:  2/ 6\t Steps:  80\t Loss: 0.379663\t Val Loss: 0.360904\t Accuracy: 0.883\n",
    "# Epoch:  2/ 6\t Steps:  90\t Loss: 0.399583\t Val Loss: 0.390520\t Accuracy: 0.857\n",
    "# Epoch:  2/ 6\t Steps: 100\t Loss: 0.467552\t Val Loss: 0.480415\t Accuracy: 0.808\n",
    "# Epoch:  3/ 6\t Steps: 110\t Loss: 0.239100\t Val Loss: 0.282348\t Accuracy: 0.896\n",
    "# Epoch:  3/ 6\t Steps: 120\t Loss: 0.091864\t Val Loss: 0.252968\t Accuracy: 0.915\n",
    "# Epoch:  3/ 6\t Steps: 130\t Loss: 0.160094\t Val Loss: 0.209478\t Accuracy: 0.934     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Plot Validation Losses\n",
    "As we've seen before with other tutorials, a great way to visualize how well a network is performing is to plot its losses (both training and testing/validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make plot\n",
    "plt.plot(epochs_list, val_losses, color=\"b\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Validation Loss of LSTM Classifier as a Function of Epochs\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the LSTM Model\n",
    "Now that we've trained our email classifer, let's test our performance on our test dataset.\n",
    "\n",
    "### 4.1 Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "S2b315Bh3QcR",
    "outputId": "f3cce04e-1af5-46e1-82ac-c26ac33e0997"
   },
   "outputs": [],
   "source": [
    "# Make counters for storing outputs from testing\n",
    "test_losses = []\n",
    "sums = []\n",
    "sizes = []\n",
    "\n",
    "# Use this to switch from \"training\" to \"evaluation\"\n",
    "net.eval()\n",
    "\n",
    "# TESTING/EVALUATION LOOP\n",
    "test_losses = []\n",
    "for seq_tensor, seq_tensor_lengths, label in iter(test_loader):\n",
    "\n",
    "    seq_tensor = seq_tensor.to(device)\n",
    "    seq_tensor_lengths = seq_tensor_lengths.to(device)\n",
    "    label = label.to(device)\n",
    "    output = net(seq_tensor, seq_tensor_lengths)\n",
    "\n",
    "    # losses\n",
    "    test_loss = criterion(output, label.float())     \n",
    "    test_losses.append(test_loss.item())\n",
    "\n",
    "    # accuracy\n",
    "    binary_output = (output >= 0.5).short() # short(): torch.int16\n",
    "    right_or_not = torch.eq(binary_output, label)\n",
    "    sums.append(torch.sum(right_or_not).float().item())\n",
    "    sizes.append(right_or_not.shape[0])\n",
    "\n",
    "accuracy = np.sum(sums) / np.sum(sizes)\n",
    "print(\"Test Loss: {:.6f}\\t\".format(np.mean(test_losses)),\n",
    "      \"Accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exercise: Try Improving the Network\n",
    "Your turn!  Try modifying the following to see if you can improve network classification accuracy:\n",
    "\n",
    "- Netork architecture (the `SpamHamLSTM` class)\n",
    "- Number of epochs\n",
    "- Learning rate\n",
    "- Optimizer\n",
    "- (If you really want to, but not recommended) Scheduler\n",
    "- (If you really want to, but not recommended) DataLoader Class\n",
    "\n",
    "For reference, the baseline accuracy that you should try to improve is **0.927**.  Good luck!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tutorial.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
