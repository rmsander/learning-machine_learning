{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batched, Multi-Input, Multi-Output Gaussian Process Regression with GPyTorch\n",
    "In this notebook, we will look at the use of batched, multi-dimensional GPyTorch (`gpytorch`) models for Gaussian Process Regression tasks.\n",
    "\n",
    "Our aim is to focus on code optimization, as well as discussing the use of certain tools such as `gc` that can be used to ensure optimal hardware performance and reassure against memory leaks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPyTorch, NumPy, Scikit-Learn, Matplotlib\n",
    "import gpytorch\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Timing and math\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import psutil\n",
    "import gc\n",
    "process = psutil.Process(os.getpid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Batched GPyTorch Model\n",
    "To model different outcomes over multi-dimensional targets, we will encode this by batching the model.  Many `gpytorch` models have batching capability in the form of specifying the keyword argument `batch_shape` (which expects a `torch.Size([x])` object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the Model\n",
    "class BatchedGP(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, shape):\n",
    "        super(BatchedGP, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([shape]))\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(batch_shape=torch.Size([shape])),\n",
    "            batch_shape=torch.Size([shape])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function\n",
    "The function below defines the function used for training our multidimensional GPyTorch model.  To batch over both the multidimensional targets/outputs as well as along the standard batch axes, we extend the first dimension by tiling the inputs by the number of output dimensions, and resizing the targets such that each block of the training and predicted targets is a dimension of the targets over a larger batch of points.\n",
    "\n",
    "Note the use of the following techniques to ensure/mitigate the possibility of GPU memory leak:\n",
    "\n",
    "1. Defining a separate function, with its own scope, for training, and calling this function.  This is similar to a `closure()` function, which you may see for other `torch` optimizers such as `LBFGS`. \n",
    "\n",
    "2. Using the garbage collection (`gc`) package, and calling `gc.collect()` at the end of the training function.\n",
    "\n",
    "3. Ensuring that anything we pull from the computational graph, e.g. `loss.item()`, is removed from the computational graph by calling `.item()` to avoid the graph persisting after it goes out of context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gp_batched_scalar(Zs, Ys, use_cuda=False, epochs=10, lr=0.1):\n",
    "    \"\"\"Computes a Gaussian Process object using GPyTorch. Each outcome is\n",
    "    modeled as a single scalar outcome.\n",
    "    \"\"\"\n",
    "    # Preprocess batch data\n",
    "    B, N, XD = Zs.shape  # B is batch dimension, N is points dimension, and XD is feature dimension\n",
    "    YD = Ys.shape[-1]  # YD is target dimension\n",
    "    \n",
    "    # Convert features to tensor, and tile according to output targets\n",
    "    train_x = torch.tensor(Zs)\n",
    "    train_x = train_x.repeat((YD, 1, 1))\n",
    "    \n",
    "    # Convert targets to tensor, and reshape from (B, N, YD) --> (B * YD, N)\n",
    "    train_y = torch.tensor(Ys)\n",
    "    train_y = train_y.view(B * YD, N)\n",
    "\n",
    "    # initialize likelihood and model - batch over output dimensions and batches\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood(batch_shape=torch.Size([B * YD]))\n",
    "    model = BatchedGP(train_x, train_y, likelihood, B * YD)\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    # If using GPU\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "        train_x = train_x.cuda()\n",
    "        train_y = train_y.cuda()\n",
    "        mll = mll.cuda()\n",
    "    \n",
    "    # Define training helper function\n",
    "    def epoch_train():\n",
    "        optimizer.zero_grad()  # Zero gradients\n",
    "        output = model(train_x)  # Compute noise-free output\n",
    "        loss = -mll(output, train_y).sum()  # Compute batched loss\n",
    "        loss.backward()  # Compute gradients with backpropagation\n",
    "        optimizer.step()  # Update weights with gradients\n",
    "        optimizer.zero_grad()  # Zero gradients\n",
    "        gc.collect()  # Used to ensure there is no memory leak\n",
    "    \n",
    "    # Run training\n",
    "    for i in range(epochs):\n",
    "        epoch_train()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    return model, likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training\n",
    "With our model, likelihood, and training routine defined, we are now ready to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "B = 256  # Batch dimension\n",
    "N = 25   # Number of points\n",
    "XD = 2   # Dimension of features\n",
    "YD = 1   # Dimension of targets\n",
    "EPOCHS = 50\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "# Create training data and labels\n",
    "train_x_np = np.random.normal(loc=0, scale=1, size=(B, N, XD))  # Create as np array\n",
    "train_y_np = np.random.normal(loc=0, scale=1, size=(B, N, YD))  # Create as np array\n",
    "train_x = torch.tensor(train_x_np).float()  # Can use .double() for 64-bit fp precision\n",
    "train_y = torch.tensor(train_y_np).float()  # Can use .double() for 64-bit fp precision\n",
    "\n",
    "# Time GPyTorch training\n",
    "model, likelihood = train_gp_batched_scalar(train_x_np, train_y_np,\n",
    "                        use_cuda=USE_CUDA, epochs=EPOCHS, lr=0.1)\n",
    "\n",
    "# Calling .eval() places the model and likelihood in \"posterior\" mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Define mll\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interreplay",
   "language": "python",
   "name": "interreplay"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
