{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Keras Logo](notebook_diagrams/keras.png)\n",
    "\n",
    "# Keras Tutorial\n",
    "\n",
    "\n",
    "Keras is a high-level neural network/deep learning package that is quite powerful - you can actually create a neural network model and train, test, and evaluate it in under 15 lines of code!  Keras is built on top of [TensorFlow](https://www.tensorflow.org/), a powerful, flexible framework that is used to build machine learning applications at scale.  While we won't study TensorFlow or PyTorch (another powerful neural network/deep learning framework) in this course, you are strongly encouraged to explore these packages as well!\n",
    "\n",
    "Keras, and other machine learning packages, leverage the modularity of object-oriented code to create multi-million parameter neural networks using very few lines of code.\n",
    "\n",
    "This Jupyter notebook uses documentation from Keras to provide a quick, hands-on introduction to this high-level machine learning API.  For reference, we used code from the following resource for the examples below: https://keras.io/.  If you'd like to look at another quick introduction to Keras, I would strongly recommend [30 Seconds to Keras](https://keras.io/#getting-started-30-seconds-to-keras), which is what this code is largely based off of.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not already installed\n",
    "!pip install keras\n",
    "import keras\n",
    "\n",
    "# OR\n",
    "from tensorflow import keras\n",
    "\n",
    "# Sequential model is very useful in Keras\n",
    "from keras.models import Sequential, model_from_json, model_from_yaml\n",
    "\n",
    "# Import different layers for defining our networks\n",
    "from keras.layers import Dense, Conv2D\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imread\n",
    "\n",
    "# For reading paths\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Data Structure in Keras: The Model\n",
    "The core data structure in Keras is the model class, which has methods for compiling with an optimizer and loss function, fitting (training), evaluation (testing), and prediction.\n",
    "\n",
    "The `model` class is one we will use for essentially all of our operations when we define, train, and test different neural network models.  For more information/documentation on Keras models, visit the link [here](https://keras.io/models/model/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward, Stacked Models from Sequential() Class\n",
    "For this tutorial and course, our focus will be primarily on using the `Sequential` type of model.  Other types of models exist, and we encourage you to investigate them!  \n",
    "\n",
    "The main idea with the `Sequential()` model is that we stack layers sequentially, one after one another.  Each layer we add to our model is cascaded with the rest of the layers in our model, by adding it to the network output.  The result we see after stacking these layers below is something similar to the picture below.\n",
    "\n",
    "![Deep NN](notebook_diagrams/deep_nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Define a Sequential Keras Model\n",
    "We can define our first model below!  We must first \"instantiate\" our Python model object, which creates the `Sequential` model object and runs the `__init__` (also known as constructor) method, which is run every time a Python object is created.\n",
    "\n",
    "Next, we can add layers one at a time.  `Dense` is a type of neural network layer corresponding to fully connected layers.  We can think of fully connected layers as a set of weights in which each node/neuron in one layer is connected to every node/neuron in the next layer.  See an example in the diagram below.\n",
    "\n",
    "![Fully Connected Layer](notebook_diagrams/fclayer.png)\n",
    "\n",
    "Notice how each node/neuron in the first layer has a weight connecting it to a node/neuron in the second layer.'  With this in mind, let's create a **fully connected neural network** below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create the model\n",
    "model = Sequential()  # Enables for stacking of layers\n",
    "\n",
    "# Now add layers\n",
    "model.add(Dense(units=64, activation='relu', input_dim=32))  # Input/first hidden layer\n",
    "model.add(Dense(units=32, activation='relu'))  # Second layer\n",
    "model.add(Dense(units=16, activation='relu'))  # Third layer\n",
    "model.add(Dense(units=12, activation='relu'))  # Fourth layer\n",
    "model.add(Dense(units=10, activation='softmax'))  # Output layer\n",
    "\n",
    "# Now get information about the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling, Training, and Evaluation\n",
    "Like other supervised learning algorithms, the next machine learning development step will be to train and test our model.  We will use our fully-connected model from above.\n",
    "\n",
    "We will first `compile` our model, which is a function used in Keras that provides our model with a loss function (which is needed for training), an optimizer (such as Stochastic Gradient Descent), and metrics over which to evaluate the model (such as accuracy or mean squared error).  Visit [this page](https://keras.io/models/model/) for more information on how the `compile` function is used.  \n",
    "\n",
    "### Loss Functions We Will Use For Keras:\n",
    "\n",
    "1. **Categorical Cross-Entropy**: Used for multi-class classification.  A network using this loss function should have the **SoftMax** activation function applied at the output.  Can be used by setting: `loss='categorical_crossentropy'`.\n",
    "\n",
    "\n",
    "2. **Binary Cross-Entropy**: Used for binary classification.  A network using this loss function should have the **sigmoid** activation function applied at the output.  Can be used by setting: `loss='categorical_crossentropy'`.  Intuition: When we have linear activation functions in the hidden layers and a **sigmoid** activation at the output, this is just logistic regression!\n",
    "\n",
    "\n",
    "3. **Mean Squared Error (MSE)**: Used for regression.  A network using this loss function should have the **Linear** or **ReLU** activation function applied at the output.  Can be used by setting: `loss='mean_squared_error'`.  Intuition: When we have linear activation functions in the hidden layers and output layer, this is just linear regression!\n",
    "\n",
    "Keras has many other **loss**, **optimizer**, and **metric** options!  We highly encourage you to investigate these on your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our model from above!\n",
    "print(model.layers)\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit model to data (x_train and y_train in this case) - data inputs need only be numpy arrays\n",
    "from keras.datasets import cifar10  # Popular image dataset\n",
    "\n",
    "# Split data into training and testing\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Cifar Data\n",
    "We'll be using the Cifar dataset for this example, which is a famous dataset that is used to train and provide a baseline evaluation for many neural network models in computer vision.\n",
    "\n",
    "Let's look at some of the data we'll be visualizing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = len(x_train)\n",
    "\n",
    "# Get random indices for showing data\n",
    "random_indices = np.random.randint(low=0, high=N, size=3)\n",
    "\n",
    "# Show images randomly\n",
    "for index in random_indices:\n",
    "    IMG = x_train[index]\n",
    "    plt.imshow(IMG)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluating Our Model\n",
    "With our data split, we are now ready to train and evaluate our neural network model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next week, we'll be diving into how our neural network models compare to some of the other models we've been analyzing in this course!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Model Specs\n",
    "We can also get model information using the specs below.  The `model` Python object contains useful information about the neural network, such as its inputs/outputs, parameters, layers, etc.  We can write a function to retrieve all of this information at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_specs(model):\n",
    "    print(model.layers)  # Flattened list of tensors comprising model\n",
    "    print(model.inputs)  # List of input tensors to model\n",
    "    print(model.outputs)  # List of output tensors of model \n",
    "    print(model.summary())  # Brief summary of your Keras model\n",
    "    print(model.get_config())  # Dict containing configuration of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Models\n",
    "Often times, we don't want to have to retrain models, and want to re-use them whenever we can.  We can do that by saving, loading, and sharing models!\n",
    "\n",
    "This is especially true for when we try to solve problems in machine learning that have been solved in a similar way before.  For example, if we wanted to train a neural network to classify between mate and coffee using images of these objects, a good place to start would be a general object detector trained on a giant dataset called ImageNet.  \n",
    "\n",
    "We won't have time to discuss it in this course, but if you're interested, training on pre-trained models for specific applications such as classifying between mate and coffee is a technique used in the field of [transfer learning](https://machinelearningmastery.com/transfer-learning-for-deep-learning/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle is also a very useful package for saving models\n",
    "import pickle\n",
    "\n",
    "# Get model information using function above\n",
    "get_model_specs(model)\n",
    "\n",
    "# Now define a path where we can save weights\n",
    "savepath = os.path.join(os.getcwd())\n",
    "\n",
    "# Save model to HDF5 file, and then reload it\n",
    "hdf5_path = os.path.join(savepath, \"hdf5_weights_ex.h5\")\n",
    "model.save_weights(hdf5_path)\n",
    "model.load_weights(hdf5_path, by_name=False) \n",
    "\n",
    "# Save as JSON representation, and then reload it\n",
    "json_string = model.to_json()\n",
    "json_pickle_fname = os.path.join(savepath, \"json_weights_ex.pkl\")\n",
    "\n",
    "with open(json_pickle_fname, \"wb\") as pkl_file:\n",
    "    pickle.dump(json_string, pkl_file)\n",
    "    pkl_file.close()\n",
    "    \n",
    "# Now reload from JSOn\n",
    "with open(json_pickle_fname, \"rb\") as pkl_file:    \n",
    "    reloaded_json = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    \n",
    "model = model_from_json(reloaded_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and Set Weights\n",
    "We can get information about weights from above, and can also set weights using our own weight files, or, more commonly, using pre-trained weights!  The intuition here is that some person/company/school went through a lot of trouble to find weights that generally work well for solving problems, so we should use them!\n",
    "\n",
    "Typically, you will use `get_weights` when saving a model, and `set_weights` when loading a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()  # Returns weights of model\n",
    "\n",
    "model.set_weights(weights)  # Sets weights of model to be weights arg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Loading Keras Datasets\n",
    "Next week, we will be analyzing the MNIST dataset, which consists of a series of handwritten digits that have labels ranging from 0 to 9.  We will briefly dive into how we can use neural networks/deep learning with Keras to develop a classifier to predict a number given a handwritten digit - a.k.a. how to train a neural network to recognize handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for \"Core\" Keras Layers\n",
    "\n",
    "Below are some of the most important parameters that can be utilized when creating the Keras `layer()` object:\n",
    "\n",
    "1. **units**: Positive integer, dimensionality of the output space.\n",
    "2. **activation**: Activation function to use (see activations). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
    "3. **use_bias**: Boolean, whether the layer uses a bias vector.\n",
    "\n",
    "The `layer()` object has other properties as well, but it is only recommended you change these if you have a strong reason to:\n",
    "\n",
    "4. **kernel_initializer**: Initializer for the kernel weights matrix (see initializers).\n",
    "5. **bias_initializer**: Initializer for the bias vector (see initializers).\n",
    "6. **kernel_regularizer**: Regularizer function applied to the kernel weights matrix (see regularizer).\n",
    "7. **bias_regularizer**: Regularizer function applied to the bias vector (see regularizer).\n",
    "8. **activity_regularizer**: Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer).\n",
    "9. **kernel_constraint**: Constraint function applied to the kernel weights matrix (see constraints).\n",
    "10. **bias_constraint**: Constraint function applied to the bias vector (see constraints)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Keras Layers\n",
    "\n",
    "Keras also supports the creation of different types of layers for different neural network applications.  For this course, we will mostly be focused on using **1, 4, 6, and 8**.    \n",
    "\n",
    "\n",
    "1. **Dense**: Fully connected layer.\n",
    "2. **Activation**: Layer for applying an activation function to an output.  \n",
    "3. **Dropout**: Applies a dropout layer.  Probability of dropout is an argument in layer init.\n",
    "4. **Flatten**: Layer typically used for turning 2D/3D into 1D vector (e.g. Conv layers to Dense).\n",
    "5. **Conv1D**: 1D convolutional layer.\n",
    "6. **Conv2D**: 2D convolutional layer.\n",
    "7. **MaxPooling1D**: Layer for max pooling in 1D.\n",
    "8. **MaxPooling2D**: Layer for max pooling in 2D.\n",
    "9. **RNN & GRU**: Base classes for recurrent layers.\n",
    "10. **BatchNormalization**: Batch normalization layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Training Specifications\n",
    "\n",
    "Keras, and deep learning as a whole, have a set of training parameters that are quite important for training models effectively.  These are:\n",
    "\n",
    "1. **learning rate**: This controls how large the steps we take during training are.  To avoid having to repeat computations too many times, we should make this as large as possible, but not so large that our weights diverge!\n",
    "\n",
    "\n",
    "2. **batch size**: This is the number of samples we process each time we take an optimization (gradient) step when training our neural network.  Generally, the larger the batch size, the more stable training will be.  Often GPU and/or CPU sizes impose limits on how large we can make our batch size.\n",
    "\n",
    "\n",
    "3. **epochs**: This is the number of complete passes through the training dataset that we make as we train our models.  Generally, more epochs leads to better performance on the training dataset.  However, we must be wary of overfitting/overtraining - just because the model performs well on the training dataset doesn't mean the model will perform will on the validation/test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Class Exercise: Build Your Own Neural Network for Binary Classification!\n",
    "\n",
    "The best way to learn how to use these packages is to try them out for yourself!  Let's go through the semantics of building a neural network model and training it.  We'll use auto-generated data to see how well our neural network can make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for generating a dataset\n",
    "def generate_features(n=10000):\n",
    "    \n",
    "    # Generate the un-rotated data\n",
    "    x1 = np.random.normal(loc=0, scale=5, size=n)\n",
    "    x2 = np.random.normal(loc=0, scale=1, size=n)\n",
    "    stacked_data = np.vstack((x1, x2))\n",
    "\n",
    "    # Generate a random angle for rotation\n",
    "    theta = np.random.randint(low=0, high=360)\n",
    "    \n",
    "    # Make rotation matrix\n",
    "    c, s = np.cos(theta), np.sin(theta)\n",
    "    R = np.array(((c,-s), (s, c)))\n",
    "\n",
    "    # Rotate data\n",
    "    return R @ stacked_data\n",
    "\n",
    "def generate_labels(D):\n",
    "    labels = np.array([1 if np.linalg.norm(D[:,i]) < 2.5 else 0 for i in range(D.shape[1])])\n",
    "    return labels\n",
    "\n",
    "def plot(D, L):\n",
    "    num_ones = len(np.nonzero(L)[0])\n",
    "    ones = np.zeros((num_ones,2))\n",
    "    zeros = np.zeros((D.shape[1]-num_ones,2))\n",
    "    \n",
    "    one_index = 0\n",
    "    zero_index = 0\n",
    "    for i in range(len(L)):\n",
    "        if L[i] == 1:\n",
    "            ones[one_index] = D[:,i]\n",
    "            one_index += 1\n",
    "\n",
    "        else:\n",
    "            zeros[zero_index] = D[:,i]\n",
    "            zero_index += 1\n",
    "    \n",
    "    l1 = plt.scatter(ones[:,0], ones[:,1], color='b')\n",
    "    l0 = plt.scatter(zeros[:,0], zeros[:,1], color='r')\n",
    "    plt.title(\"Graph of Binary Elliptical Dataset\")\n",
    "    plt.legend((l1, l0), (\"Label = 1\", \"Label = 0\"))\n",
    "    plt.show()\n",
    "    \n",
    "def split_train_test(D, L, test_split=0.2, VAR=1):\n",
    "    # Get index for splitting data\n",
    "    split_num = int((1-test_split) * D.shape[1])\n",
    "    \n",
    "    # Split features into training and testing, and add noise to testing\n",
    "    D_train = D[:,:split_num]\n",
    "    D_test = D[:,split_num:]\n",
    "    \n",
    "    # Generate radial Gaussian noise\n",
    "    N = D_test.shape[1]\n",
    "    noise = np.random.normal(loc=0, scale=VAR, size=D_test.shape)\n",
    "    D_test = np.add(D_test, noise)\n",
    "    \n",
    "    \n",
    "    # Split labels into training and testing\n",
    "    L_train = L[:split_num]\n",
    "    L_test = L[split_num:]\n",
    "    \n",
    "    # Return all data\n",
    "    return D_train, D_test, L_train, L_test\n",
    "\n",
    "\n",
    "# Generate dataset and labels\n",
    "D = generate_features()\n",
    "L = generate_labels(D)\n",
    "D_train, D_test, L_train, L_test = split_train_test(D, L)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Data\n",
    "We can visualize these different classes using `matplotlib`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting Train Data\")\n",
    "plot(D_train, L_train)\n",
    "\n",
    "print(\"Plotting Test Data\")\n",
    "plot(D_test, L_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Turn\n",
    "Can you design a neural network to learn this representation?  Some starting code is provided.  We'll compare how well your prediction does both quantiatively and visually.\n",
    "\n",
    "What accuracies can you achieve on this test set?  Experiment with the following model and model parameters:\n",
    "\n",
    "1. `layers`: You can change the number and types of layers in the model specification.\n",
    "\n",
    "\n",
    "2. `EPOCHS`: This is the number of complete passes we make through the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, initialize model\n",
    "my_model = Sequential()\n",
    "\n",
    "# Now, add layers to your network! HINT: Make sure you use \"activation=sigmoid\" at your output layer\n",
    "my_model.add(Dense(units=32, activation='relu', input_dim=2))  # Input/first hidden layer\n",
    "my_model.add(Dense(units=1, activation='sigmoid'))  # Input/first hidden layer\n",
    "\n",
    "\"\"\"<ADD MORE LAYERS HERE>\"\"\"\n",
    "\n",
    "# Compile the model - what loss should we use?  Remember we're doing 1/0 classification.\n",
    "LOSS = \"binary_crossentropy\"\n",
    "my_model.compile(loss=LOSS,\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy', 'binary_accuracy'])\n",
    "\n",
    "# Now train the model!\n",
    "EPOCHS = 10 #\"\"\"<SET EPOCHS HERE>\"\"\"\n",
    "\n",
    "# Now we are ready to fit our model!\n",
    "my_model.fit(D_train.T, L_train.T, epochs=EPOCHS, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the training dataset\n",
    "loss_and_metrics = my_model.evaluate(D_test.T, L_test.T, batch_size=128)\n",
    "\n",
    "# Find predictions on the test dataset\n",
    "classes = my_model.predict(D_test.T, batch_size=128)\n",
    "classes[classes >= 0.5] = 1\n",
    "classes[classes < 0.5] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now compare plots to see how we did!\n",
    "print(\"Predicted classes\")\n",
    "plot(D_test, classes)\n",
    "\n",
    "print(\"Actual classes\")\n",
    "plot(D_test, L_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one peculiar thing about this dataset that you probably won't find with many other datasets.  Do you know what it is?  Hint: Think about how our samples were generated (independently and identically-distributed).  Does the split we have for our training and testing datasets affect the distribution of each?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADDITIONAL CONTENT: Modifying the Model() Class for Fully-Customizable Models\n",
    "We can modify the `Model()` class in Keras to create flexible models with our choice of layers, actvation functions, etc.  \n",
    "\n",
    "The code below is more similar to the code we would see when building models in `pytorch` or `tensorflow`, which are machine learning packages that provide the user more flexibility with defining models and training and evaluation procedures, at the price of being somewhat more lower-level.  We will not be using these frameworks in this course, but we strongly encourage you to explore these frameworks in greater detail if you're interested.  Both `pytorch` and `tensorflow` are free and open-source, so you could start using them right now if you wanted to!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "class SimpleMLP(keras.Model):\n",
    "    \n",
    "    # This function is known as the \"constructor\" for class objects\n",
    "    def __init__(self, use_bn=False, use_dp=False, num_classes=2):\n",
    "        \n",
    "        # \"Inherits from the base keras.Model class\"\n",
    "        super(SimpleMLP, self).__init__(name='mlp')\n",
    "        self.use_bn = use_bn\n",
    "        self.use_dp = use_dp\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Define layers of neural network model\n",
    "        # EXERCISE: CHANGE THIS!\n",
    "        #########################################################\n",
    "        self.dense1 = keras.layers.Dense(32, activation='relu')\n",
    "        self.dense2 = keras.layers.Dense(16, activation='relu')\n",
    "        self.dense3 = keras.layers.Dense(4, activation='relu')\n",
    "        self.dense4 = keras.layers.Dense(1, activation='sigmoid')\n",
    "        if self.use_dp:\n",
    "            self.dp = keras.layers.Dropout(0.5)\n",
    "        if self.use_bn:\n",
    "            self.bn = keras.layers.BatchNormalization(axis=-1)\n",
    "        #########################################################\n",
    "    \n",
    "    # Function for passing an input through the neural network\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        if self.use_dp:\n",
    "            x = self.dp(x)\n",
    "        if self.use_bn:\n",
    "            x = self.bn(x)\n",
    "        return self.dense2(x)\n",
    "\n",
    "# Create a model, compile it with a loss function, optimizer, and metric, and then train it!\n",
    "simple_model = SimpleMLP()  # NOTE - You can change parameters here!\n",
    "simple_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy', 'binary_accuracy'])\n",
    "\n",
    "# Fit model to training data\n",
    "simple_model.fit(D_train.T, L_train.T, epochs=10)\n",
    "\n",
    "# Evaluate the model on the training dataset\n",
    "loss_and_metrics = simple_model.evaluate(D_test.T, L_test.T, batch_size=128)\n",
    "\n",
    "# Find predictions on the test dataset\n",
    "my_new_classes = simple_model.predict(D_test.T, batch_size=128)\n",
    "my_new_classes[classes >= 0.5] = 1\n",
    "my_new_classes[classes < 0.5] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now compare plots to see how we did!\n",
    "print(\"Predicted classes\")\n",
    "\n",
    "plot(D_test, my_new_classes)\n",
    "\n",
    "print(\"Actual classes\")\n",
    "plot(D_test, L_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
