{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PyTorch](notebook_diagrams/pytorch.jpg)\n",
    "\n",
    "# Tutorial for PyTorch\n",
    "PyTorch, like TensorFlow, is a deep learning library for creating scalable deep learning models that are efficient and compact.  It is also widely used across deep learning applications, and offers a great balance of creating highly-flexible machine learning frameworks with making code interpretable and easy to understand.\n",
    "\n",
    "The key element behind PyTorch is also the computation graph. PyTorch, unlike TensorFlow, utilizes only [eager execution](https://medium.com/coding-blocks/eager-execution-in-tensorflow-a-more-pythonic-way-of-building-models-e461810618c8) (so you don't have to worry about issues with integrating numpy, printing variables, etc.).\n",
    "\n",
    "This tutorial is derived from a combination of tutorials from PyTorch.  See the reference [here](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html).\n",
    "\n",
    "### What is PyTorch?\n",
    "\n",
    "Itâ€™s a Python-based scientific computing package targeted at two sets of\n",
    "audiences:\n",
    "\n",
    "-  A framework that's similar to numpy, but enables you to better utilize the power of GPUs\n",
    "-  a deep learning research platform that provides maximum flexibility\n",
    "   and speed\n",
    "   \n",
    "Let's explore what PyTorch can do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install PyTorch\n",
    "We'll use Anaconda to install PyTorch on our AWS machines for this tutorial.  If you don't want to install this package through Anaconda, you can also do so through `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate conda environment\n",
    "! conda activate local_env\n",
    "\n",
    "# Install PyTorch in Conda environment\n",
    "! conda install -c pytorch pytorch\n",
    "! pip install torchvision\n",
    "\n",
    "# Check PyTorch version\n",
    "! pip show torch\n",
    "\n",
    "# Use matplotlib inline version\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import PyTorch\n",
    "Quick note here: notice how we will use `import torch`, not `import pytorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For an explanation on the future module: https://stackoverflow.com/questions/7075082/what-is-future-in-python-used-for-and-how-when-to-use-it-and-how-it-works/7075121\n",
    "from __future__ import print_function\n",
    "\n",
    "# Import PyTorch package and modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Import numpy for later\n",
    "import numpy as np\n",
    "\n",
    "# Import matplotlib for graphing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyTorch Tensors\n",
    "Also like TensorFlow, the key data structure with PyTorch is the tensor.  Remember that these tensors are very similar to the numpy `nd_array` data structures we saw when we were learning `numpy`.\n",
    "\n",
    "The diagram below also shows us another way we can think about [PyTorch tensors](https://www.datacamp.com/community/tutorials/investigating-tensors-pytorch).\n",
    "\n",
    "![Tensor Intuition](notebook_diagrams/tensor_diagram.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tensor Examples\n",
    "For each of these exercises, think about a potential equivalent operation we could use for numpy `nd_arrays`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.1 Construct a 5x3 matrix, uninitialized**.  Think about filling this matrix with null/empty values.  Recall, especially with tensors, that it's computationally less expensive to allocate space in memory for the `tensor` all at once, rather than gradually increasing the size it needs over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensor of empty values\n",
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.2 Construct a randomly initialized matrix**.  This kind of operation can be helpful if we are trying to generate probabilistic data from a distribution, without using a numpy wrapper.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensor of random numbers\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.3 Construct a matrix filled zeros and of dtype long**.  Think about filling this matrix with null/empty values.  Recall, especially with tensors, that it's computationally less expensive to allocate space in memory for the `tensor` all at once, rather than gradually increasing the size it needs over time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensor of zeros\n",
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.4 Construct a tensor directly from data**.  You can think of this operation as placing a pytorch `tensor` wrapper on the native Python list.  From a performance perspective, operations like these can be important for numpy, pytorch, and tensorflow, because they can make numerical computations on data types such as lists much more quickly and efficiently than native Python can, especially if the user has access to GPUs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensor directly from list\n",
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.5 Construct a tensor directly from another tensor**.  With PyTorch, you can also create a tensor based on an existing tensor. These methods will reuse properties of the input tensor, e.g. dtype, unless new values are provided by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensor directly from another tensor\n",
    "x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizes\n",
    "print(x)\n",
    "\n",
    "# Create tensor directly from another tensor\n",
    "x = torch.randn_like(x, dtype=torch.float)    # override dtype!\n",
    "print(x)                                      # result has the same size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access the size of any tensor in PyTorch through the `size()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The size of the tensor x is: %s\" % (str(x.size())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 More Tensor Operations\n",
    "Like the numpy library, we also have access to many pytorch operations that we can use for numerical computations on `tensor` objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Tensor Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: tensor + tensor\n",
    "y = torch.rand(5, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: torch.add(tensor, tensor)\n",
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: torch.add(tensor, tensor) > result\n",
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 4: In place (Any operation that mutates a tensor in-place is post-fixed with an ``_``.)\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Indexing and Resizing\n",
    "Indexing in pytorch is quite similar to indexing in numpy.  We'll find that a lot of the same functionalities we used before, such as:\n",
    "\n",
    "- Multidimensional slicing\n",
    "- Conditional indexing\n",
    "\n",
    "Are also helpful for manipulating pytorch `tensor` objects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resizing\n",
    "If you want to resize/reshape tensor, you can use ``torch.view``:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tensor of random numbers with shape --> (4, 4)\n",
    "x = torch.randn(4, 4)\n",
    "\n",
    "# Convert shape of random tensor to shape --> (16, 1)\n",
    "y = x.view(16)\n",
    "\n",
    "# Can also \"infer dimensions from other dimensions\" --> (2, 8)\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "\n",
    "# Compare results\n",
    "print(\"Shape of x: \\n %s, \\n \\n, Shape of y: \\n %s \\n \\n, Shape of z: \\n %s \\n \\n\" % \n",
    "      (x.size(), y.size(), z.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting Items from Tensors\n",
    "If you have a one element tensor, use ``.item()`` to get the value as a\n",
    "Python number.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-element tensor\n",
    "x = torch.randn(1)\n",
    "\n",
    "# Print tensor and item\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 More Functions\n",
    "Like numpy, in addition to the operations discussed in this tutorial, pytorch offers 100+ Tensor operations, including transposing, indexing, slicing, mathematical operations, linear algebra, random numbers, etc., are described [here](https://pytorch.org/docs/torch).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Relating PyTorch to NumPy: NumPy Bridge\n",
    "\n",
    "Converting a Torch Tensor to a NumPy array and vice versa is a breeze.\n",
    "\n",
    "The Torch Tensor and NumPy array will share their underlying memory\n",
    "locations (if the Torch Tensor is on CPU), and changing one will change\n",
    "the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Interchange between tensor and nd_array\n",
    "Below, we'll convert a Torch Tensor to a NumPy Array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random PyTorch tensor\n",
    "a = torch.ones(5)\n",
    "print(\"PyTorch tensor: %s\" % (a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert random PyTorch tensor to numpy nd_array\n",
    "b = a.numpy()\n",
    "print(\"Numpy array: %s\" % (b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the numpy array changed in value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a numpy array\n",
    "a = np.ones(5)\n",
    "\n",
    "# Convert to a pytorch tensor\n",
    "b = torch.from_numpy(a)\n",
    "\n",
    "# Now convert back to numpy automatically\n",
    "np.add(a, 1, out=a)\n",
    "\n",
    "# numpy array\n",
    "print(a)\n",
    "\n",
    "# pytorch tensor\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the Tensors on the CPU except a CharTensor support converting between NumPy and PyTorch tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Networks/Deep Learning in PyTorch\n",
    "The best features of PyTorch are its functionalities for efficiently and compactly creating, training, and evaluating complicated neural networks.\n",
    "\n",
    "Neural networks can be constructed using the ``torch.nn`` package.  These networks use functionality known as `autograd` ([automatic differentiation](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)) that automatically computes gradients used for training.  We won't need to use it explicitly, but it's something to be mindful of when understanding what's going on when we train neural networks.\n",
    "\n",
    "An `nn.Module` object contains layers, and a method for predicting outputs from inputs: `forward(input)` that returns the `output` predicted value.\n",
    "\n",
    "A typical training procedure for a neural network is as follows:\n",
    "\n",
    "1. Define the neural network that has some learnable parameters (or weights)\n",
    "2. Iterate over a dataset of inputs\n",
    "3. Process input through the network\n",
    "4. Compute the loss (how far is the output from being correct)\n",
    "5. Propagate gradients back into the networkâ€™s parameters\n",
    "6. Update the weights of the network, typically using a simple update rule:\n",
    "  ``weight = weight - learning_rate * gradient``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Let's Make a Neural Network in PyTorch!\n",
    "We can now use the ``nn.Module`` object to create our own neural network in PyTorch!  This method is similar to \"sub-classing\" (modifying the defaults of an object to introduce custom features) models in TensorFlow and Keras to create our own custom models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):  # We'll call this object \"net\"\n",
    "\n",
    "    # This is the constructor method!  This \"initializes\" the object when we make it.\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Use this call to inherit from the super class, which runs the constructor for nn.Module\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Now we can add our own customizable features\n",
    "        # Network specification:\n",
    "        #     input image channel, 6 output channels, 3x3 square convolution kernel\n",
    "        \n",
    "        # Convolution layers\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        \n",
    "        # Make fully connected layers\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension \n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    # This is the method we use to make predictions from our input \"x\" to our output.\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    # We can also define other custom methods here\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "# Instantiate our neural network!  This creates an object according to the definitions above.\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating neural network objects, all you need to do is define the `__init__` (constructor) and `forward` functions/class methods (note: you can define other methods as well, but it's not necessary to for training).  Once you define the two class methods above, the `backward`\n",
    "function (where gradients are computed during training) is **automatically** defined for you\n",
    "using `autograd`.  You can use any of the Tensor operations in the `forward` function.\n",
    "\n",
    "After defining the architecture and forward methods of our network, we can view the trainable parameters of this network by calling the default class method `net.parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of parameters\n",
    "params = list(net.parameters())\n",
    "\n",
    "# Get number of parameters\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Making Neural Network Predictions in PyTorch\n",
    "Let's try making a prediction using our network on a random 32x32 input.\n",
    "Note: expected input size of this net (LeNet) is 32x32. To use this net on\n",
    "the MNIST dataset, please resize the images from the dataset to 32x32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the random input\n",
    "x = torch.randn(1, 1, 32, 32)\n",
    "\n",
    "# Make the network prediction and print it\n",
    "out = net(x)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Zeroing Gradients in PyTorch\n",
    "One small nuance of PyTorch is that by default, gradients will accumulate.  You will need to \"zero\" them whenever you make another update step.\n",
    "\n",
    "Below, we'll zero the gradient buffers of all parameters and backprops with random gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero the gradients\n",
    "net.zero_grad()\n",
    "\n",
    "# Calling the backward method propagates gradients backward\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: `torch.nn` only supports mini-batches. The entire `torch.nn`\n",
    "    package only supports inputs that are a mini-batch of samples, and not\n",
    "    a single sample.\n",
    "\n",
    "    For example, `nn.Conv2d` will take in a 4D Tensor of\n",
    "    ``nSamples x nChannels x Height x Width``.\n",
    "\n",
    "    If you have a single sample, just use `input.unsqueeze(0)` to add\n",
    "    a fake batch dimension\n",
    "\n",
    "Before proceeding further, let's recap all the classes youâ€™ve seen so far.\n",
    "\n",
    "**Recap:**\n",
    "  -  `torch.Tensor` - A *multi-dimensional array* with support for autograd\n",
    "     operations like ``backward()``. Also *holds the gradient* w.r.t. the\n",
    "     tensor.\n",
    "     \n",
    "  -  `nn.Module` - Neural network module. *Convenient way of\n",
    "     encapsulating parameters*, with helpers for moving them to GPU,\n",
    "     exporting, loading, etc.\n",
    "     \n",
    "  -  `nn.Parameter` - A kind of Tensor, that is *automatically\n",
    "     registered as a parameter when assigned as an attribute to a*\n",
    "     `Module`.\n",
    "     \n",
    "  -  `autograd.Function` - Implements *forward and backward definitions\n",
    "     of an autograd operation*. Every ``Tensor`` operation creates at\n",
    "     least a single `Function` node that connects to functions that\n",
    "     created a `Tensor` and *encodes its history*.\n",
    "\n",
    "**At this point, we covered:**\n",
    "  -  Defining a neural network\n",
    "  -  Processing inputs and calling backward\n",
    "\n",
    "**Still Left:**\n",
    "  -  Computing the loss\n",
    "  -  Updating the weights of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Loss Functions in PyTorch\n",
    "A loss function takes the (output, target) pair of inputs, and computes a\n",
    "value that estimates how far away the output is from the target.\n",
    "\n",
    "There are several different\n",
    "[loss functions](https://pytorch.org/docs/nn.html#loss-functions) under the\n",
    "nn package.\n",
    "A simple loss is: `nn.MSELoss` which computes the mean-squared error\n",
    "between the input and the target.  Let's look at the example below to see this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a network prediction\n",
    "output = net(x)\n",
    "\n",
    "# Set a \"target\" value (this is a \"label\" in supervised learning)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "\n",
    "# Reshape the target\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Compute loss and return it\n",
    "loss = criterion(output, target)\n",
    "printable_loss = str(loss.detach().numpy())\n",
    "print(\"Loss is: %s\" % (printable_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you follow `loss` in the backward direction, using its\n",
    "`.grad_fn` attribute, you will see a set of computations that look\n",
    "like this:\n",
    "\n",
    "    input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "          -> view -> linear -> relu -> linear -> relu -> linear\n",
    "          -> MSELoss\n",
    "          -> loss\n",
    "\n",
    "So, when we call `loss.backward()`, all tensors that have `requires_grad=True`\n",
    "will have their `.grad` Tensor accumulated with the gradient.  **This is how PyTorch stores gradients for the back-propagation algorithm!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Backpropagation in PyTorch\n",
    "\n",
    "To backpropagate the error all we have to do is call `loss.backward()`.\n",
    "**You need to clear the existing gradients though, else gradients will be\n",
    "accumulated to existing gradients.** You can do this by calling `net.zero_grad()`.\n",
    "\n",
    "\n",
    "Now we shall call `loss.backward()`, and have a look at conv1's bias\n",
    "gradients before and after the backward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero gradients before running back-propagation\n",
    "net.zero_grad()     \n",
    "\n",
    "# Show gradients before weight update\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "# Run the backpropagation step\n",
    "loss.backward()\n",
    "\n",
    "# Show gradients after weight update\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have seen how to use loss functions.\n",
    "\n",
    "**Read Later:**\n",
    "\n",
    "  The neural network package contains various modules and loss functions\n",
    "  that form the building blocks of deep neural networks. A full list with\n",
    "  documentation is [here](https://pytorch.org/docs/nn).\n",
    "\n",
    "**The only thing left to learn is:**\n",
    "\n",
    "  - Updating the weights of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Weight Updates in PyTorch\n",
    "\n",
    "The simplest update rule used in practice is the Stochastic Gradient\n",
    "Descent (SGD).  Under this update rule, we have that:\n",
    "\n",
    "`weight = weight - learning_rate * gradient`\n",
    "    \n",
    "However, as you use neural networks, you want to use various different\n",
    "update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.\n",
    "To enable this, we built a small package: ``torch.optim`` that\n",
    "implements all these methods.  Typically, the recommended optimizer to use is [ADAM](https://pytorch.org/docs/stable/_modules/torch/optim/adam.html).  Using these optimizers is simple - let's investigate this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import torch optimizers module\n",
    "import torch.optim as optim\n",
    "\n",
    "# Create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# In your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "\n",
    "# Make network predictions \n",
    "output = net(x)\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(output, target)\n",
    "printable_loss = str(loss.detach().numpy())\n",
    "print(\"Loss is: %s\" % (printable_loss))\n",
    "\n",
    "# Take gradient update step\n",
    "loss.backward()\n",
    "\n",
    "# Make the gradient update\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: A reminder that zeroing the gradients is extremely important for ensuring that your gradients don't accumulate!  This should be done every time you make a new weight update step (one of the guides below will show you how this fits into neural network training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Specifying Operation Modes for Neural Network Models\n",
    "One of the last nuanced features of PyTorch is that we will need to tell our network when we want to be considered in \"training mode\" and in \"evaluation mode\".  These commands are simple; supposing that our neural network is called `Net`, we only need to call:\n",
    "\n",
    "- **Training**: `Net.train()`\n",
    "- **Evaluation**: `Net.eval()`\n",
    "\n",
    "The recommended way to include these statements in your code is to have `Net.train()` right before the beginning of your **training** loop, and `Net.eval()` right before the beginning of your **testing/evaluation** loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Devices in PyTorch\n",
    "While TensorFlow contains separate installations for CPU and GPU-based packages, PyTorch does not.  This means  you will need to tell PyTorch what device you plan to make computations through (by default, PyTorch assumes this is CPU).  **This is easier than you think!**\n",
    "\n",
    "To explain how we tell our machine which device to use, we'll introduce [CUDA (Compute Unified Device Architecture)](https://en.wikipedia.org/wiki/CUDA), a parallel processing platform and Application Programming Interface (API) developed by NVIDIA.  We'll use this package frequently with PyTorch when we want to make computations with our GPU.\n",
    "\n",
    "**NOTE**: Though PyTorch's runtime performance can be significantly improved by using parallel processing with multiple CPUs/GPUs, it is not necessary for you to (though you should definitely consider doing so if you're planning to deploy any applications with PyTorch).  PyTorch will still perform at the same level in terms of accuracy regardless of the device we use.\n",
    "\n",
    "![CUDA](notebook_diagrams/cuda.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Check if CUDA is available on your machine\n",
    "CUDA is usually available (already installed or installation is possible) if your device contains GPU capabilities.  We can check this below.\n",
    "\n",
    "If CUDA is available, we can create tensors and send them to the GPU, and then do computations on the GPU.  Two ways in which we can do this:\n",
    "\n",
    "- Create a tensor directly on the GPU by specifying the `device=device` argument when we create `tensor` objects.\n",
    "- Create a tensor on the CPU, and then move it to the GPU with the `.to()` tensor method.\n",
    "\n",
    "**NOTE**: If your machine does not have CUDA, you can install it by following the installation link [here](https://developer.nvidia.com/cuda-downloads), and selecting your correct operating system.  Note that for our AWS machines, we are using **Linux**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    \n",
    "    # Check where cuda is, and store as a CUDA device object\n",
    "    device = torch.device(\"cuda\")          \n",
    "    \n",
    "    # Directly create a pytorch tensor on the \n",
    "    y = torch.ones_like(x, device=device)  \n",
    "    \n",
    "    # Or send device to GPU with \"tensor.to(device)\"\n",
    "    x = x.to(device)\n",
    "    \n",
    "    # Do tensor computation on GPU\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    \n",
    "    # Send tensor from GPU --> CPU, and change dtype\n",
    "    print(z.to(\"cpu\", torch.double))       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Why Should We Care About Devices?\n",
    "Computing with PyTorch objects **can** (but won't always) be accelerated significantly with GPU-computing, which is why we should consider moving our `tensor` objects to and from the GPU.  \n",
    "\n",
    "**NOTE**: It's important to keep in mind that most GPUs have significantly less memory than CPUs, so oftentimes, especially if we're working with large datasets, it is not possible to store the entire dataset on the GPU, and we'll need to add it incrementally.  This is another reason why PyTorch has compact methods and functions for moving data between devices.\n",
    "\n",
    "Below is a diagram for a typical PyTorch workflow with **GPU-based computing**.  This uses the following three steps:\n",
    "\n",
    "- Create a tensor or batch of tensors in CPU memory.\n",
    "- Move tensor or batch of tensors in CPU memory to GPU memory.  Make numerical computations on GPU tensor(s).\n",
    "- Move tensor or batch of tensors back from GPU memory to CPU memory.\n",
    "\n",
    "![CUDA Workflow](notebook_diagrams/cuda_workflow.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Parallel Processing and GPUs are Great, But Do They Always Work?\n",
    "Somemtimes, more devices does not mean better performance.  This can happen for a variety of different reasons, but this usually happens because the GPUs cannot take advantage of parallel computation.  \n",
    "\n",
    "When using more advanced devices such as single or multiple GPUs, it is important to compare the performance of your numerical computations with different device configurations (such as CPU-only or GPU-only).  The code below can help us do just that (the reference for this code snippet can be found [here](https://discuss.pytorch.org/t/cpu-x10-faster-than-gpu-recommendations-for-gpu-implementation-speed-up/54980))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run code block if CUDA available\n",
    "if torch.cuda.is_available():\n",
    "    # Make start and end timer objects\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # Start timer\n",
    "    start.record()\n",
    "\n",
    "    # Put numerical computations here\n",
    "    y = torch.rand(5, 3)\n",
    "    x = torch.rand(5, 3)\n",
    "    z = x + y\n",
    "\n",
    "    # End timer\n",
    "    end.record()\n",
    "\n",
    "    # Compute total execution time\n",
    "    torch.cuda.synchronize()\n",
    "    execution_time = start.elapsed_time(end)\n",
    "    print(\"Execution time is: %s\" % (execution_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Examples When PyTorch GPU is Slower/Faster than GPU\n",
    "Below, we will show examples for when PyTorch is slower/faster using a GPU as compared to using a CPU.  In the case where it is faster (increasing the batch size), it is because the additional complexity we are introducing is highly parallelizable, whereas in the case where we simply increase the variability of the data we analyze, we see that using a GPU can actually result in slower performance.  It's important to be mindful of what likely will, and will not, result in improved runtime performance with GPUs.\n",
    "\n",
    "An example when using a GPU results in **slower** runtime (**more complex data**):\n",
    "![GPU Fast](notebook_diagrams/gpu_slow.png)\n",
    "\n",
    "An example when using a GPU results in **faster** runtime (**larger batch size**):\n",
    "![GPU Fast](notebook_diagrams/gpu_fast.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "In this tutorial, we've introduced the deep learning library **PyTorch**, a flexible and fast Python library for creating compact, effective deep learning models and training them.  Some of the most important concepts we discussed were:\n",
    "\n",
    "- **tensors**: PyTorch's data structure for storing data and making computations on it.  These `tensor` objects are almost identical to numpy `nd_array` objects, and many operations we can use for `tensor` objects are quite similar to other operations we've seen.\n",
    "\n",
    "\n",
    "- **devices**: PyTorch can be used on different devices (such as CPUs and GPUs).  The Compute Unified Device Architecture (CUDA) interface/API can be used with PyTorch to work with different devices that are available to users.  Users can also transfer data to and from the CPU and GPU.\n",
    "\n",
    "\n",
    "- **models**: Neural networks are the central functionality for PyTorch.  These are defined in a similar way as we saw for Keras and TensorFlow.  The core methods we need to define for these models are the `__init__` (constructor) method, and the `forward` method (for prediction).\n",
    "\n",
    "\n",
    "- **predictions, loss, and updates**: We showed you how to make predictions, calculate losses, and make weight updates according to gradient steps.  The next tutorials will show you how these all fit into training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Link to PyTorch Exercises\n",
    "The best way to master PyTorch is to practice!  We've posted some additional exercises that you should work through if you would like to gain more experience using PyTorch.  The exercises are as follows:\n",
    "\n",
    "- `pytorch-tutorial-image-classifier.ipynb` is a tutorial that walks through how to train an image classifier using PyTorch.  All of the work is done for you, but we highly encourage you to work through it to understand how all the elements of tensors, models, devices, prediction, and training are tied together to train models that do tasks such as image classification.  **This tutorial can be found [here](pytorch-tutorial-image-classifier.ipynb).**\n",
    "\n",
    "\n",
    "- `pytorch-tutorial-LSTM.ipynb` is a tutorial that walks through how to train a [Long Short-Term Memory (LSTM)](https://en.wikipedia.org/wiki/Long_short-term_memory) neural network, a kind of neural network that is most commonly used for Natural Language Processing applications.  In this tutorial, you will use the LSTM to make an email spam detector.  **This tutorial can be found [here](pytorch-tutorial-LSTM.ipynb).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Where do I go next?\n",
    "If you finish the exercises above, there are also many more exercises, tutorials, and concepts to learn through the [PyTorch website](https://pytorch.org/), and we highly encourage you to look into these as well!  Once you feel comfortable with this library, try to use PyTorch to start solving problems that you think deep learning can help improve!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
